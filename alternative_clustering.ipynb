{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filippo_Valle \n",
      "last updated: Tue Jun 30 2020 \n",
      "\n",
      "CPython 3.7.6\n",
      "IPython 7.15.0\n",
      "\n",
      "pandas 1.0.4\n",
      "numpy 1.18.5\n",
      "seaborn 0.10.1\n",
      "tensorflow 2.2.0\n",
      "sklearn 0.0\n",
      "scipy 1.4.1\n",
      "\n",
      "compiler   : GCC 7.5.0\n",
      "system     : Linux\n",
      "release    : 4.19.76-linuxkit\n",
      "machine    : x86_64\n",
      "processor  : x86_64\n",
      "CPU cores  : 2\n",
      "interpreter: 64bit\n",
      "Git hash   : 728c5fcb2ae65fb2d06e3b61b8b167d388b13f95\n",
      "Git repo   : git@github.com:fvalle1/topics.git\n",
      "Git branch : develop\n",
      "watermark 2.0.2\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark -v -m  -u -n -p pandas,numpy,seaborn,tensorflow,sklearn,scipy -a Filippo_Valle -g -r -b -w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt  \n",
    "import pandas as pd  \n",
    "import numpy as np \n",
    "import seaborn as sns\n",
    "import os, sys\n",
    "from hsbmpy import get_file, define_labels, get_cluster_given_l, get_max_available_L\n",
    "from geneontology import topic_analysis\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import homogeneity_completeness_v_measure\n",
    "import scipy.cluster.hierarchy as shc\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from lda import lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level = 2\n",
    "directory=r\"/home/jovyan/work/phd/datasets/paper/gtexhk\"\n",
    "#L=get_max_available_L(directory)\n",
    "os.chdir(directory)\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"mainTable.csv\", index_col=[0], header=[0]).dropna()\n",
    "totalobjcets = len(df.columns)\n",
    "print(df.info())\n",
    "print(\"Maximum expression value: %.1e\"%df.max().max())\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_files = pd.read_csv(\"files.dat\", index_col=[0])\n",
    "df_files.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_out = []\n",
    "label = \"SMTS\"\n",
    "if label not in df_files.columns:\n",
    "    raise AttributeError(f\"{label} not Avaliable\")\n",
    "for sample in df.columns.values:\n",
    "    try:\n",
    "        true_out.append(get_file(sample, df_files)[label])\n",
    "    except:\n",
    "        print(*sys.exc_info())\n",
    "        true_out.append('unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open(\"clustersizes.txt\",'r') as f:\n",
    "        xl = np.sort(np.array(f.read().split(sep='\\n'))[:-1].astype(int))\n",
    "except:\n",
    "        xl=np.linspace(2,50,3, dtype=int)\n",
    "xl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xl=xl[:-1]\n",
    "xl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hierarchical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure()\n",
    "dend = shc.dendrogram(shc.linkage(np.log2(df.T.values+1), method='average'), leaf_rotation=90., leaf_font_size=8.,)\n",
    "plt.xlabel(\"samples\", fontsize=16)\n",
    "plt.show()\n",
    "fig.savefig(\"hierarchical_dendogram.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hierarchical\n",
    "scores['hierarchical']={\n",
    "    'h':[],\n",
    "    'c':[],\n",
    "    'V':[]\n",
    "}\n",
    "def pearson_affinity(M, parallel=True):\n",
    "    return 1. - np.array([[pearsonr(a,b)[0] for a in M] for b in M])\n",
    "\n",
    "print(\"hierarchical-log\")\n",
    "os.system('mkdir -p hierarchical-log')\n",
    "hierarchical_model = AgglomerativeClustering(n_clusters=1, affinity='euclidean', linkage='complete')  \n",
    "for l,x in enumerate(xl):\n",
    "    print(\"testing with %d clusters\"%x)\n",
    "    hierarchical_model.n_clusters=x\n",
    "    data = np.log2(1.+df.T.values)\n",
    "    #data = df.T.values\n",
    "    out = hierarchical_model.fit_predict(data)\n",
    "        \n",
    "    #save clusters\n",
    "    print(\"saving clusters\")\n",
    "    df_clusters = pd.DataFrame(index=np.arange(len(df.columns)))\n",
    "    for c in np.arange(out.max()+1)[::-1]:\n",
    "        c_objects = df.columns[np.argwhere(out==c)].T[0]\n",
    "        df_clusters.insert(0,\"Cluster %d\"%(c+1),np.concatenate((c_objects,[np.nan for _ in np.arange(len(df.columns)-len(c_objects))])))\n",
    "    df_clusters.dropna(axis=0,how='all', inplace=True)\n",
    "    df_clusters.to_csv(\"hierarchical-log/hierarchical-log_level_%d_clusters.csv\"%(l), index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"topicsizes.txt\",'r') as f:\n",
    "    tl = np.array(f.read().split(sep='\\n'))[:-1].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xl = xl[:3]\n",
    "print(xl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "model=lda(n_jobs=4, verbose=2,  max_iter=5)\n",
    "model.full_analysis(directory, xl, tl=None, logarithmise=False, label=\"SMTS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.wrappers import LdaMallet\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.test.utils import get_tmpfile\n",
    "from gensim.corpora import MmCorpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=list(map(list,map(lambda x: list(zip(range(len(df.index)), df[x])),df.columns)))\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary=Dictionary([df.index])\n",
    "[a for a in dictionary.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_file=get_tmpfile(\"/home/fvalle/phd/datasets/tcga/oversampling_10tissue/corpus.mm\")\n",
    "MmCorpus.serialize(out_file, corpus, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LdaMallet(\"/home/fvalle/phd/Mallet/bin/mallet\", workers=5, corpus=corpus, num_topics=15, id2word=dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topics=pd.DataFrame(data=model.get_topics().T, index=[a[1] for a in dictionary.items()], columns=[\"Topic %d\"%(t+1) for t in range(5)])\n",
    "df_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topic_distr=pd.read_csv(model.fdoctopics(), sep='\\t', header=None, index_col=0).drop(1,1)\n",
    "df_topic_distr.columns=[\"Topic %d\"%(t+1) for t in range(5)]\n",
    "df_topic_distr.index=df.columns\n",
    "df_topic_distr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topic_distr.apply(lambda x: x.idxmax().split(\" \")[1], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hiermodel = AgglomerativeClustering(n_clusters=10, affinity='euclidean', linkage='ward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('clustersizes.txt') as f:\n",
    "    xl=np.array(f.read().split('\\n')[:-1]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(\"mkdir -p hierhsbm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_out = []\n",
    "for sample in pd.read_csv(\"%s/%s_level_%d_topic-dist.csv\"%('topsbm','topsbm',0), index_col=1).drop('i_doc', axis=1).index.values:\n",
    "    try:\n",
    "        true_out.append(get_file(sample, df_files)['primary_site'])\n",
    "    except:\n",
    "        print(sys.exc_info()[0])\n",
    "        true_out.append('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores['hierhsbm']={\n",
    "    'h':[],\n",
    "    'c':[],\n",
    "    'V':[]\n",
    "}\n",
    "for l,n_clusters in enumerate(xl):\n",
    "    print(\"Fitting level %d with %d clusters\"%(l, n_clusters))\n",
    "    df_topics = pd.read_csv(\"%s/%s_level_%d_topic-dist.csv\"%('topsbm','topsbm',l), index_col=1).drop('i_doc', axis=1)\n",
    "    df_clusters = pd.DataFrame(columns=[\"Cluster %d\"%c for c in np.arange(n_clusters)+1])\n",
    "    hiermodel.n_clusters=n_clusters\n",
    "    out = hiermodel.fit_predict(df_topics.values)  \n",
    "    for c in np.arange(out.max()+1)[::-1]:\n",
    "        c_objects = df_topics.index[np.argwhere(out==c)].values.T[0]\n",
    "        df_clusters[\"Cluster %d\"%(c+1)]=np.concatenate((c_objects,[np.nan for _ in np.arange(len(df_topics.index)-len(c_objects))]))\n",
    "    df_clusters.dropna(axis=0,how='all', inplace=True)\n",
    "    df_clusters.to_csv(\"hierhsbm/hierhsbm_level_%d_clusters.csv\"%(l), index=False, header=True)\n",
    "    #metrics\n",
    "    print(\"saving metrics\")\n",
    "    score = (homogeneity_completeness_v_measure(true_out, out))\n",
    "    scores['hierhsbm']['h'].append(score[0])\n",
    "    scores['hierhsbm']['c'].append(score[1])\n",
    "    scores['hierhsbm']['V'].append(score[2])\n",
    "    \n",
    "pd.DataFrame(data=scores['hierhsbm']).to_csv(\"%s/hierhsbm.scores\"%directory, header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"mainTable.csv\", index_col=0)\n",
    "df.info()\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"corpus.txt\",'w') as file:\n",
    "    for sample in df.columns:\n",
    "        for g in np.array(df[sample].sort_values(ascending=False).index[:1000],dtype=str):\n",
    "            file.write(g[:15])\n",
    "            file.write(\" \")\n",
    "        file.write(\"\\n\")\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions on how to install it are available from its authors\n",
    "[https://amaral.northwestern.edu/resources/software/topic-mapping](https://amaral.northwestern.edu/resources/software/topic-mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(\"export PATH=$PATH:/home/jovyan/work/topicmapping/bin\")\n",
    "os.system(\"topicmap -f corpus.txt -r 10 -t 10 -seed 42 -o tm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_words = pd.read_csv(\"tm/word_wn_count.txt\", sep=' ', header=None)\n",
    "df_words.columns=['word', 'word-id', 'occurrence']\n",
    "df_words.sort_values('word-id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topic_distr = pd.read_csv(\"tm/lda_gammas_final.txt\", sep=' ', header=None)\n",
    "df_topic_distr.columns=['Topic %d'%(t+1) for t in df_topic_distr.columns]\n",
    "df_topic_distr.index.name='i_doc'\n",
    "df_topic_distr.insert(0,'doc',df.columns)\n",
    "df_topic_distr=df_topic_distr.dropna(how='all',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = df_topic_distr.drop('doc',1).values.argmax(1)\n",
    "df_clusters=pd.DataFrame()\n",
    "for cluster in range(np.max(clusters)+1):\n",
    "    elems=df.columns[clusters==cluster].values\n",
    "    df_clusters.insert(0,'Cluster %d'%(cluster+1),np.concatenate([elems, ['' for _ in range(len(df.columns)- len(elems))]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clusters.sort_index(axis=1).to_csv(\"tm/tm_level_0_clusters.csv\", index=False, header=True)\n",
    "df_topic_distr.to_csv(\"tm/tm_level_0_topic-dist.csv\", index=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_distr = pd.DataFrame().fillna(0)\n",
    "with open(\"tm/lda_betas_sparse_final.txt\",\"r\") as f:\n",
    "    for line in f.read().split(\"\\n\"):\n",
    "        row = line.split(\" \")\n",
    "        if len(row) < 2:\n",
    "            continue\n",
    "        topic = int(row[0])+1\n",
    "        line=np.array(row[1:-1], dtype=float).reshape(int((len(row)-1)/2),2)\n",
    "        for el in line:\n",
    "            df_word_distr.at[df_words[df_words['word-id']==int(el[0])].word.values[0], f\"Topic {topic}\"] = el[1]\n",
    "#df_word_distr.index=df_words['word']\n",
    "df_word_distr.fillna(0)\n",
    "df_word_distr.to_csv(\"tm/tm_level_0_word-dist.csv\", index=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topics = pd.DataFrame()\n",
    "max_L = df_word_distr.shape[0]\n",
    "for topic in df_word_distr.columns[::-1]:\n",
    "    t_series = df_word_distr[topic]\n",
    "    t_series = t_series[t_series>t_series.quantile(0.99)]\n",
    "    df_topics.insert(0,topic,np.concatenate((t_series.index.values,np.repeat(np.nan, df_word_distr.shape[0]-len(t_series)))))\n",
    "df_topics.dropna(how=\"all\", axis=0).to_csv(\"tm/tm_level_0_topics.csv\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WGCNA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use [WGCNA.ipynb](WGCNA.ipynb) to run analyses and come back here to adapt data to next analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in range(5):\n",
    "    df_wgcna = pd.read_csv(\"wgcna/wgcna_level_%d_labels.csv\"%l, index_col=0)\n",
    "    totalobjcets = len(df_wgcna.index)\n",
    "    out = df_wgcna['x'].values\n",
    "    print(\"saving clusters\")\n",
    "    df_clusters = pd.DataFrame(index=np.arange(totalobjcets))\n",
    "    for c in np.arange(out.max()+1)[::-1]:\n",
    "        c_objects = df_wgcna.index[np.argwhere(out==(c+1))].T[0]\n",
    "        df_clusters.insert(0,\"Cluster %d\"%(c+1),np.concatenate((c_objects,[np.nan for _ in np.arange(totalobjcets-len(c_objects))])))\n",
    "    df_clusters.dropna(axis=0,how='all', inplace=True)\n",
    "    df_clusters.to_csv(\"wgcna/wgcna_level_%d_clusters.csv\"%l, index=False, header=True)\n",
    "df_wgcna_td = pd.read_csv(\"wgcna/wgcna_level_0_topic-dist.csv\")\n",
    "df_wgcna_td.columns.values[0]='doc'\n",
    "df_wgcna_td.index.name='i_doc'\n",
    "df_wgcna_td.to_csv(\"wgcna/wgcna_level_0_topic-dist.csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_distr = pd.read_csv(\"wgcna/wgcna_level_0_word-dist.csv\", index_col=0)\n",
    "df_topics = pd.DataFrame()\n",
    "max_L = df_word_distr.shape[0]\n",
    "for topic in df_word_distr.columns[::-1]:\n",
    "    t_series = df_word_distr[topic]\n",
    "    t_series = t_series[t_series>0.5]\n",
    "    df_topics.insert(0,topic.replace(\"MM\",\"ME\"),np.concatenate((t_series.index.values,np.repeat(np.nan, df_word_distr.shape[0]-len(t_series)))))\n",
    "df_topics.dropna(how=\"all\", axis=0).to_csv(\"wgcna/wgcna_level_0_topics.csv\", index=False, header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
